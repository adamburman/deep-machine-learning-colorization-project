{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a3985f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Caroline Andersson\" \n",
    "NAME2 = \"Adam Burman\"\n",
    "GROUP = \"84\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ea77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check Python version\n",
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','9'), \"You are not running Python 3.9. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec2e61",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456aa90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = r\"C:\\Users\\carol\\deep-machine-learning\\project\\Dataset\"\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path\n",
    "import torchvision.transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "#import colorspacious as cs\n",
    "\n",
    "\n",
    "import kornia # https://kornia.readthedocs.io/en/latest/color.html\n",
    "#Lab color is computed using the D65 illuminant and Observer 2. (deafult in kornia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813fdc9",
   "metadata": {},
   "source": [
    "### 1.2 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf68f95",
   "metadata": {},
   "source": [
    "Make a dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576071b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously GTAData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a76855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from itertools import chain\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "class GTALabData(Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(self, root, transform = None):\n",
    "            \"\"\"Constructor\n",
    "        \n",
    "        Args:\n",
    "            root (Path/str): Filepath to the data root\n",
    "            transform (Compose): A composition of image transforms.\n",
    "        \"\"\"\n",
    "            root = Path(root)\n",
    "            if not (root.exists() and root.is_dir()):\n",
    "                raise ValueError(f\"Data root '{root}' is invalid\")\n",
    "            \n",
    "            self.root = root\n",
    "            self.transform = transform\n",
    "            self.img_paths, self.label_paths = self._collect_samples()\n",
    "            \n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get sample by index\n",
    "        \n",
    "        Args:\n",
    "            index (int)\n",
    "        \n",
    "        Returns:\n",
    "             The index'th sample Tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        img_path = self.img_paths[index]\n",
    "        label_path = self.label_paths[index]\n",
    "\n",
    "        # Load the image and label into memory\n",
    "        img = Image.open(img_path)\n",
    "        #label = Image.open(label_path)\n",
    "        img = torchvision.transforms.functional.pil_to_tensor(img) # PIL --> Tensor\n",
    "        img = kornia.color.rgb_to_lab(img/255) # RGB --> Lab\n",
    "        \n",
    "        L = img[0]\n",
    "        a = img[1]\n",
    "        b = img[2]\n",
    "\n",
    "        # Perform transforms, if any.\n",
    "        if self.transform is not None:\n",
    "            L = self.transform(L.numpy()) # transform L\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        ab = torch.cat((a.unsqueeze(0), b.unsqueeze(0)), dim=0)\n",
    "        #L = torch.cat((L.unsqueeze(0), L.unsqueeze(0), L.unsqueeze(0)), dim=0).squeeze()\n",
    "\n",
    "        return L, ab\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Get the number of samples in the dataset\"\"\"\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def _collect_samples(self):\n",
    "        \"\"\"Collect all paths and labels\n",
    "        \n",
    "        Helper method for the constructor\n",
    "        \"\"\"\n",
    "        \"\"\"Collect all paths and labels\n",
    "\n",
    "        Helper method for the constructor\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Get image and label paths\n",
    "        img_dir = self.root / \"01_images\" / \"images\"\n",
    "        label_dir = self.root / \"01_labels\" / \"labels\"\n",
    "        \n",
    "        # check if there are images in the directory\n",
    "        #self._check_images_in_directory(img_dir)\n",
    "        #self._check_images_in_directory(label_dir)\n",
    "\n",
    "        img_paths = list(img_dir.glob(\"*.png\"))\n",
    "        label_paths = list(label_dir.glob(\"*.png\"))\n",
    "\n",
    "        if len(img_paths) != len(label_paths):\n",
    "            raise ValueError(\"Number of images and labels must be the same\")\n",
    "            \n",
    "        return img_paths, label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ba0fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "torch.Size([1, 1052, 1914])\n",
      "torch.Size([2, 1052, 1914])\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose\n",
    "transform = Compose([ToTensor()])\n",
    "\n",
    "example_dataset = GTALabData(path, transform)\n",
    "print(len(example_dataset)) #1\n",
    "L, ab = example_dataset[2]\n",
    "#print(img)\n",
    "print(L.shape)\n",
    "print(ab.shape)\n",
    "print(np.max(L.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4ae89",
   "metadata": {},
   "source": [
    "### 1.3 Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c987d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize, RandomRotation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resize_size = 224\n",
    "mean = 52.37087932\n",
    "std = 24.64118381\n",
    "'''\n",
    "# Calculate mean and standard deviation for normalization\n",
    "number_of_samples = len(example_dataset)\n",
    "pixelValues = []\n",
    "mean = [0.0] #52.37087932 \n",
    "std = [0.0] #24.64118381\n",
    "for image,_ in example_dataset:\n",
    "    imgArray = np.array(image)\n",
    "    mean += np.mean(imgArray) / number_of_samples\n",
    "    std += np.std(imgArray) / number_of_samples\n",
    "print(mean)\n",
    "print(number_of_samples)\n",
    "'''\n",
    "# ToTensor scales data between mean, std which is needed later\n",
    "transform_data = Compose([ToTensor(), Resize((resize_size, resize_size), antialias=True), \n",
    "                          Normalize(mean, std, inplace=False)])\n",
    "\n",
    "img_dataset_transformed = GTALabData(path, transform_data)\n",
    "test_image_transformed, test_label = img_dataset_transformed[2]\n",
    "#print(img_dataset_transformed)\n",
    "#display_image(test_image, test_label) # obs! has to do the same thing on label and image!\n",
    "\n",
    "#print(test_image[test_image<0]) # check for negative values since all must be between 0 and 1 \n",
    "#(div by 255 automatically in ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622f28f",
   "metadata": {},
   "source": [
    "### 1.4 Augument and Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d809a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m mean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m]\n\u001b[0;32m      9\u001b[0m std \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image,_ \u001b[38;5;129;01min\u001b[39;00m example_dataset:\n\u001b[0;32m     11\u001b[0m     imgArray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[0;32m     12\u001b[0m     mean \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(imgArray) \u001b[38;5;241m/\u001b[39m number_of_samples\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mGTALabData.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     40\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#label = Image.open(label_path)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpil_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# PIL --> Tensor\u001b[39;00m\n\u001b[0;32m     43\u001b[0m img \u001b[38;5;241m=\u001b[39m kornia\u001b[38;5;241m.\u001b[39mcolor\u001b[38;5;241m.\u001b[39mrgb_to_lab(img\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m) \u001b[38;5;66;03m# RGB --> Lab\u001b[39;00m\n\u001b[0;32m     45\u001b[0m L \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:207\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(nppic)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    208\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:758\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[0;32m    756\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m--> 758\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import RandomGrayscale, ColorJitter, RandomHorizontalFlip, Resize, Normalize, RandomRotation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resize_size = 224\n",
    "'''\n",
    "# Calculate mean and standard deviation for normalization\n",
    "number_of_samples = len(example_dataset)\n",
    "pixelValues = []\n",
    "mean = [0.0]\n",
    "std = [0.0]\n",
    "for image,_ in example_dataset:\n",
    "    imgArray = np.array(image)\n",
    "    mean += np.mean(imgArray) / number_of_samples\n",
    "    std += np.std(imgArray) / number_of_samples\n",
    "print(mean)\n",
    "print(number_of_samples)\n",
    "'''\n",
    "\n",
    "# ToTensor scales data between mean, std which is needed later\n",
    "transform_data = Compose([ToTensor(), Resize((resize_size, resize_size), antialias=True), \n",
    "                          Normalize(mean, std, inplace=False)])\n",
    "\n",
    "img_dataset_transformed = GTALabData(path, transform_data)\n",
    "test_image_transformed, test_label = img_dataset_transformed[2]\n",
    "#print(img_dataset_transformed)\n",
    "#display_image(test_image, test_label) # obs! has to do the same thing on label and image!\n",
    "\n",
    "#print(test_image[test_image<0]) # check for negative values since all must be between 0 and 1 \n",
    "#(div by 255 automatically in ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 52.37087932\n",
    "std = 24.64118381\n",
    "print(np.max(test_image_transformed.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13465bbe",
   "metadata": {},
   "source": [
    "### 1.5 Chop up into train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Calculate mean std\n",
    "temp_dataset = GTALabData(path, ToTensor())\n",
    "\n",
    "#test_img,_ = gta_dataset[0]\n",
    "#print(test_img)\n",
    "'''\n",
    "number_of_samples = len(temp_dataset)\n",
    "dataset_mean = []\n",
    "dataset_std = []\n",
    "for image,_ in temp_dataset:\n",
    "    imgArray = np.array(image)\n",
    "    #print(imgArray)\n",
    "    dataset_mean.append(np.mean(imgArray))\n",
    "    dataset_std.append(np.std(imgArray))\n",
    "\n",
    "print(np.min(dataset_mean))\n",
    "dataset_mean = np.mean(dataset_mean)\n",
    "dataset_std = np.mean(dataset_std)\n",
    "'''\n",
    "mean = 52.37087932\n",
    "std = 24.64118381\n",
    "img_size = 32#224\n",
    "#gta_transform = Compose([ToTensor(), Resize((img_size, img_size), antialias=True), Normalize(mean, std, inplace=False)]) \n",
    "gta_transform = Compose([ToTensor(), Resize((img_size, img_size), antialias=True)])\n",
    "gta_dataset = GTALabData(path, gta_transform)\n",
    "\n",
    "test_img, ab = gta_dataset[0]\n",
    "print(np.max(test_img.numpy()))\n",
    "print(np.min(test_img.numpy()))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "number_of_samples = len(gta_dataset)\n",
    "dataset_mean = []\n",
    "dataset_std = []\n",
    "for image,_ in gta_dataset:\n",
    "    imgArray = np.array(image)\n",
    "    #print(imgArray)\n",
    "    dataset_mean.append(np.mean(imgArray))\n",
    "    dataset_std.append(np.std(imgArray))\n",
    "\n",
    "print(np.max(dataset_mean))\n",
    "print(np.min(dataset_mean))\n",
    "    \n",
    "dataset_mean = np.mean(dataset_mean)\n",
    "dataset_std = np.mean(dataset_std)\n",
    "    \n",
    "#print(dataset_mean)\n",
    "print(number_of_samples)\n",
    "\"\"\"\n",
    "\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "n_samples = len(gta_dataset)\n",
    "n_val_samples = int(n_samples * val_ratio)\n",
    "n_test_samples = int(n_samples * test_ratio)\n",
    "n_train_samples = n_samples - n_val_samples - n_test_samples\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    gta_dataset, [n_train_samples, n_val_samples, n_test_samples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b78a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "img_nr = 200\n",
    "L, ab = train_img_dataset_transformed[img_nr]\n",
    "print(ab.shape)\n",
    "print(L.shape)\n",
    "L=L.numpy()\n",
    "a=ab[0].numpy()\n",
    "b=ab[1].numpy()\n",
    "#display_image(L)\n",
    "#display_image(a,b)\n",
    "#print(torch.max(b))\n",
    "_, axis = plt.subplots(1, 3)\n",
    "        \n",
    "        \n",
    "height, width = L.shape\n",
    "axis[0].imshow(L, cmap = 'gray')\n",
    "axis[0].set_xlim(0, width)\n",
    "axis[0].set_ylim(height, 0)\n",
    "axis[0].set_title('L')\n",
    "        \n",
    "height, width = a.shape\n",
    "axis[1].imshow(a, cmap = 'RdYlGn')\n",
    "axis[1].set_xlim(0, width)\n",
    "axis[1].set_ylim(height, 0)\n",
    "axis[1].set_title('a')\n",
    "        \n",
    "height, width = b.shape\n",
    "axis[2].imshow(b, cmap = 'viridis')\n",
    "axis[2].set_xlim(0, width)\n",
    "axis[2].set_ylim(height, 0)\n",
    "axis[2].set_title('b')\n",
    "#plt.imshow(my_tensor.numpy()[0])\n",
    "plt.show()\n",
    "# in the opposite direction L is in range [0, 100], a,b is in range [-128, 127]\n",
    "\n",
    "display_image(test_image)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cf3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b471bb",
   "metadata": {},
   "source": [
    "##  2. Create an Autoenconder for the Raw images\n",
    "vgg16 can be used as the encoder by using transfer learning with the gtaV images. Then a decoder has to be constructed from scratch to generate a new image. The raw images are used in this section and later on another autoencoder will be trained with segmented images (our labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233edc1",
   "metadata": {},
   "source": [
    "### 2.1 Create a Head for vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12588af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "vgg_model = models.vgg16(weights = models.VGG16_Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23108dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 1000, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxUnpool2d(kernel_size=3, stride=2, padding=0),\n",
    "            nn.ConvTranspose2d(1000, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32, 2, kernel_size=3, padding=1),  # Output has 2 channels (ab color channels)\n",
    "            nn.Tanh()  # Output values between -1 and 1\n",
    "        )\n",
    "        '''\n",
    "        self.tc1 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.tc2 = nn.ConvTranspose2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.tc3 = nn.ConvTranspose2d(256, 32, kernel_size=3, padding=1)\n",
    "        self.tc4 = nn.ConvTranspose2d(32, 2, kernel_size=3, padding=1)  # Output has 2 channels (ab color channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        print(x.shape)\n",
    "        ab_channels = self.decoder(x)\n",
    "        print(ab_channels.shape)\n",
    "        #ab_channels = 127*ab_channels \n",
    "        '''\n",
    "        print(x.shape)\n",
    "        x = self.tc1(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "         \n",
    "        x = self.tc2(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.tc3(x)\n",
    "        print(x.shape)\n",
    "        #x = nn.Tanh(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Endocer\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 32, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, padding=1)  # Output has 2 channels (ab color channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        #Decoder\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.tc1 = nn.ConvTranspose2d(32, 256, kernel_size=3, padding=1)\n",
    "        self.tc2 = nn.ConvTranspose2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.tc3 = nn.ConvTranspose2d(512, 2, kernel_size=3, padding=1)  # Output has 2 channels (ab color channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        \n",
    "        x = self.tc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.tc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.tc3(x)\n",
    "        print(x)\n",
    "        \n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78625776",
   "metadata": {},
   "source": [
    "### 2.2 Freeze and Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17627c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = nn.Sequential(vgg_model.features, VGG_head())\n",
    "print(base_model.parameters)\n",
    "# Freeze all the parameters in the VGG16 model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss, train_acc = train_epoch(model,\n",
    "                                                   optimizer,\n",
    "                                                   loss_fn,\n",
    "                                                   train_loader,\n",
    "                                                   val_loader,\n",
    "                                                   device,\n",
    "                                                   print_every)\n",
    "        val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Train acc.: {sum(train_acc)/len(train_acc):.3f}, \"\n",
    "              f\"Val. loss: {val_loss:.3f}, \"\n",
    "              f\"Val. acc.: {val_acc:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        train_accs.extend(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    return model, train_losses, train_accs, val_losses, val_accs\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device, print_every):\n",
    "    # Train:\n",
    "    model.train()\n",
    "    train_loss_batches, train_acc_batches = [], []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, labels = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model.forward(inputs)\n",
    "        loss = loss_fn(z, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "        hard_preds = output_to_label(z)\n",
    "        acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "        train_acc_batches.append(acc_batch_avg)\n",
    "\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss, val_acc = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                  f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tTrain acc.: {sum(train_acc_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \"\n",
    "                  f\"\\tVal. acc.: {val_acc:.3f}\")\n",
    "\n",
    "    return model, train_loss_batches, train_acc_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    val_acc_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, labels = x.to(device), y.to(device)\n",
    "            z = model.forward(inputs)\n",
    "\n",
    "            batch_loss = loss_fn(z, labels.float())\n",
    "            val_loss_cum += batch_loss.item()\n",
    "            hard_preds = output_to_label(z)\n",
    "            acc_batch_avg = (hard_preds == labels).float().mean().item()\n",
    "            val_acc_cum += acc_batch_avg\n",
    "    return val_loss_cum/len(val_loader), val_acc_cum/len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "batch_size = 4\n",
    "base_model = AutoEncoder()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(base_model.parameters(), lr =0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = 1\n",
    "print_every = 1\n",
    "\n",
    "\n",
    "    \n",
    "first_model, first_train_losses, first_train_accs, first_val_losses, first_val_accs= training_loop(base_model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c62ae",
   "metadata": {},
   "source": [
    "### 2.3 Create a Decoder (outputs a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74718ca",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0f790",
   "metadata": {},
   "source": [
    "## 3. Create a Network for Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5d2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ffe934",
   "metadata": {},
   "source": [
    "## 4. Create an Autoencoder that uses the Network  in 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e92a20",
   "metadata": {},
   "source": [
    "Combine Semantic segmentation with a new autoencoder to colorize images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5440254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd900daa",
   "metadata": {},
   "source": [
    "## 5. Compare the Two Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08034f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
